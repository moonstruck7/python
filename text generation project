import numpy
import sys
import nltk
nltk.download('stopwords')
from nltk.tokenize import RegexpTokenizer
from nltk.corpus import stopwords
from keras.models import Sequential
from keras.layers import Dense, Dropout, LSTM
from keras.utilis np_utils
from keras.callbacks import ModelCheckpoint

file = open("frankenstein-2.txt").read()

def tokenize_words(input):
  input = input.lower()
  tokenizer = RegexpTokenizer(r'\wt')
  tokens = tokenizer.tokenize(input)
  filtered = filter(lamda token: token not in stopwords.words('english'),tokens)
  return "".join(filtered)
processed_inputs = tokenize_words(file)

chars = sorted(list(set(processed_inputs)))
char_to_num = dict((c,i) for i, c in enumerate(chars))

input_len = len(processed_inputs)
vocab_len = len(chars)
print ("Total number of characters:", input_len)
print ("Total vocab:", vocab_len)

seq_length = 100
x_data = []
y_data = []


for i in range(0, input_len - seq_length, l):
  in_seq = processed_inputs[i:i + seq_length]
  out_seq = processed_inputs[i + seq_length]
  x_data,appen([char_to_num[char] for char in in_seq])
  y_data.append({char_num[out_seq])

n_patterns = len(x_data)
print ("Total 
